# -*- coding: utf-8 -*-
"""modeling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ww7AaC0FfMmsd9e6DsN2nZ82s4QKHMrS
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
df=pd.read_csv('/content/drive/MyDrive/result2 - result2.csv')

df=df.drop(['Unnamed: 0'],axis=1)
df=df.drop(['title'],axis=1)

df

df=pd.get_dummies(df,columns=['thumbnail_link'])

df=pd.get_dummies(df,columns=['categoryId'])

df

df.shape

"""#- **모델 돌리기**"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import *
X = df.drop(['trend'], axis='columns')
y =  df.iloc[:,6]#trend
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=10)
#모델링
def modeling(model,x_train,x_test,y_train,y_test):
    model.fit(x_train,y_train)
    pred = model.predict(x_test)
    metrics(y_test,pred)
#평가 지표
def metrics(y_test,pred):
    accuracy = accuracy_score(y_test,pred)
    precision = precision_score(y_test,pred)
    recall = recall_score(y_test,pred)
    f1 = f1_score(y_test,pred)
    roc_score = roc_auc_score(y_test,pred,average='macro')
    print('정확도 : {0:.2f}, 정밀도 : {1:.2f}, 재현율 : {2:.2f}'.format(accuracy,precision,recall))
    print('f1-score : {0:.2f}, auc : {1:.2f}'.format(f1,roc_score,recall))

from sklearn.datasets import make_classification
from sklearn.decomposition import PCA
from imblearn.over_sampling import SMOTE

# 모델설정
sm = SMOTE(ratio='auto', kind='regular')

# train데이터를 넣어 복제함
X_train_over, y_train_over = sm.fit_sample(X_train,y_train)

print('After OverSampling, the shape of train_X: {}'.format(X_train_over.shape))
print('After OverSampling, the shape of train_y: {} \n'.format(y_train_over.shape))

print("After OverSampling, counts of label '1': {}".format(sum(y_train_over==1)))
print("After OverSampling, counts of label '0': {}".format(sum(y_train_over==0)))

from sklearn.linear_model import LogisticRegression
import numpy as np
lr = LogisticRegression()
modeling(lr,X_train_over,X_test,np.ravel(y_train_over),y_test)

"""# -**앙상블**"""

from sklearn.ensemble import RandomForestClassifier

rnd=RandomForestClassifier(random_state=0)

rf_clf = RandomForestClassifier(random_state=0) 

modeling(rf_clf,X_train_over,X_test,np.ravel(y_train_over),y_test)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score # 정확도 함수

clf = RandomForestClassifier(n_estimators=20, max_depth=5,random_state=0)
clf.fit(X_train,y_train)

predict1 = clf.predict(X_test)
print(accuracy_score(y_test,predict1))

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score # 정확도 함수

clf = RandomForestClassifier(n_estimators=100, max_depth=100,random_state=0)
clf.fit(X_train,y_train)

predict1 = clf.predict(X_test)
print(accuracy_score(y_test,predict1))

import matplotlib.pyplot as plt
import numpy as np

def plot_feature_importances_cancer(model):
    n_features = X_train.shape[1]
    plt.barh(range(n_features//2), model.feature_importances_[:n_features // 2], align='center')
    plt.yticks(np.arange(n_features//2), X_train.columns[:n_features // 2])
    plt.xlabel("feature importances")
    plt.ylabel("features")
    plt.ylim(-1, n_features//2)
    plt.figure(figsize=(20, 16))
    plt.show()
    plt.barh(range(n_features - n_features//2), model.feature_importances_[n_features // 2:], align='center')
    plt.yticks(np.arange(n_features - n_features//2), X_train.columns[n_features // 2:])
    plt.xlabel("feature importances")
    plt.ylabel("features")
    plt.ylim(-1, n_features - n_features//2)
    plt.figure(figsize=(20, 16))
    plt.show()

# 특성 중요도
print("특성 중요도 : \n{}".format(clf.feature_importances_[:10]))

# 특성 중요도 시각화 하기
plot_feature_importances_cancer(clf)

"""# **인공신경망**"""

from keras import models
from keras import layers
from keras import optimizers
from tensorflow.keras.layers import(Dense, Dropout, BatchNormalization)
import keras
model = keras.Sequential()
model.add(BatchNormalization())
model.add(layers.Dense(128, activation='relu'))
model.add(Dropout(0.10))
model.add(layers.Dense(64, activation='relu'))
model.add(Dropout(0.10))
model.add(layers.Dense(32, activation='relu'))
model.add(Dropout(0.10))
model.add(layers.Dense(16, activation='relu'))
model.add(Dropout(0.50))
model.add(layers.Dense(1, activation='sigmoid'))

import keras
opt=keras.optimizers.Adam(learning_rate=0.005)
model.compile(optimizer='rmsprop',
             loss='binary_crossentropy',
             metrics=['binary_accuracy'])

model.fit(X_train_over,y_train_over,epochs=100)

_,accuracy=model.evaluate(X_test,y_test)
print('Accuracy: %.2f' %(accuracy*100))

model2 = keras.Sequential()
model2.add(BatchNormalization())

model2.add(layers.Dense(64, activation='relu'))
model2.add(Dropout(0.10))
model2.add(layers.Dense(32, activation='relu'))
model2.add(Dropout(0.50))
model2.add(layers.Dense(1, activation='sigmoid'))

opt=keras.optimizers.Adam(learning_rate=0.005)
model2.compile(optimizer='rmsprop',
             loss='binary_crossentropy',
             metrics=['binary_accuracy'])

model2.fit(X_train_over,y_train_over,epochs=100)

_,accuracy=model2.evaluate(X_test,y_test)
print('Accuracy: %.2f' %(accuracy*100))